<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://rknambiar.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://rknambiar.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-08T05:40:49+00:00</updated><id>https://rknambiar.github.io/feed.xml</id><title type="html">Rohit Nambiar</title><subtitle>My personal website.
</subtitle><entry><title type="html">Vision Transformers</title><link href="https://rknambiar.github.io/blog/2023/vit/" rel="alternate" type="text/html" title="Vision Transformers" /><published>2023-08-29T14:14:00+00:00</published><updated>2023-08-29T14:14:00+00:00</updated><id>https://rknambiar.github.io/blog/2023/vit</id><content type="html" xml:base="https://rknambiar.github.io/blog/2023/vit/"><![CDATA[<p>Let’s explore vision transformers!</p>

<h1 id="background">Background</h1>]]></content><author><name></name></author><category term="attention" /><category term="deep-learning" /><category term="transformers" /><category term="computer-vision" /><summary type="html"><![CDATA[what's up with vision transformers]]></summary></entry><entry><title type="html">paying attention to attention</title><link href="https://rknambiar.github.io/blog/2023/attention/" rel="alternate" type="text/html" title="paying attention to attention" /><published>2023-08-15T14:14:00+00:00</published><updated>2023-08-15T14:14:00+00:00</updated><id>https://rknambiar.github.io/blog/2023/attention</id><content type="html" xml:base="https://rknambiar.github.io/blog/2023/attention/"><![CDATA[<p>We explore the attention mechanism used in sequence to sequence networks. Let’s go!</p>

<hr />

<h2 id="neural-machine-translation">Neural Machine Translation</h2>

<p>It’s 2014 and we begin with the task of Neural Machine Translation (NMT). Unlike statistical machine translation, NMT aims at building a single neural network that can be jointly learned to maximize the translation performance. These models generally have an encoder-decoder architecture where the encoder converts the input sequence to a fixed length vector, which the decoder uses as its context for generating the output sequence.</p>

<div class="col-sm mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-rnn-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-rnn-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-rnn-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-rnn.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<div class="caption">
    Encoder-Decoder architecture for neural machine translation
</div>

<p>Here, the encoder units are in blue, decoder units in green, input sequences in red and output sequences in purple. Input \(x_{0}, x_{1} ... x_{n}\) is fed sequentially to the encoder with the output of the encoder at time \(t_{n}\) depending on hidden state \(h_{n-1}\) and input \(x_{n}\). The output of the encoder \(h_{n}\) or \(s_{0}\) captures the input as a fixed length vector and is fed as input to the decoder. The decoder generates the first output token given \(s_{0}\) and <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> token. The output of the decoder at \(t_{1}\) ie. \(y^{`}_{1}\) is fed as input to the decoder at time step \(t_{2}\) along with the previous hidden state. This is done till a <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> token is generated. Now, during training, the generated input can be gibberish, so we need a good way to compute the loss and train the network. Instead of providing the previous time-step output, we provide the ground-truth label \(y_{i-1}\) as input for time-step \(t_{i}\).</p>

<p>The drawback of compressing all of the input information in a fixed length vector is that the network fails to perform well for long sentences. The performance deteriorates rapidly as the length of the sentence increases <d-cite key="bahdanau2014neural"></d-cite>.</p>

<hr />

<h2 id="attention-based-models">Attention-based Models</h2>

<p>Attention-based models address the above problem by <i>looking</i> at the entire input sequence before generating a new word. As mentioned in <d-cite key="bahdanau2014neural"></d-cite>, each time the model generates a new word, it (soft-)searches for a set of positions in a source sentence where most relevant information is concentrated. The model then predicts the target word based on the context vectors and the previous generated target word. This is shown in the figure below (blue block). Let’s explore this a bit.</p>

<div class="col-sm mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-rnn-attention-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-rnn-attention-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-rnn-attention-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-rnn-attention.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<div class="caption">
    Encoder-Decoder architecture with attention
</div>

<h3 id="attention-block">Attention block</h3>

<p>Let’s explore the attention block in a bit more detail. The inputs to this block while decoding at time \(t\) are the encoder hidden states \((h_1, h_2, ... h_n)\) and the previous decoder hidden state \(s_{t-1}\). The output is a context-vector \(c_t\) which is input along with the input token to the decoder. This is shown in the figure below</p>

<div class="row justify-content-md-center">
    <div class="col col-8">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-attention-block-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-attention-block-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-attention-block-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-attention-block.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Fig. Attention block
</div>

<p>First, we compute scores (\(e_{ij}\)) using the alignment model.</p>

<p>\begin{equation}
e_{ij} = a(s_{j-1}, h_j)
\end{equation}</p>

<p>It tells us how well the inputs around position \(j\) and the output at position \(i\) match. Then we pass the scores through a softmax function to compute the weight \(\alpha_{ij}\) of each \(h_j\).</p>

<p>\begin{equation}
\alpha_{ij} = \dfrac{exp(e_{ij})}{\sum_{k=1}^{T_x} exp(e_{ik})}
\end{equation}</p>

<p>The context vector \(c_i\) is then computed as the weighted sum of the annotations \(h_i\)</p>

<p>\begin{equation}
c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_i
\end{equation}</p>

<p>Although in the diagram above, we show only for one context vector, this operation is done for each step of the decoder generation phase. Section 3.1 of <d-cite key="bahdanau2014neural"></d-cite> has in detail description of what these values mean and how one can intuitively think of them.</p>

<h3 id="global-vs-local-attention">Global v/s Local attention</h3>

<p>In <d-cite key="luong2015effective"></d-cite>, the authors explore two different types of attention mechanisms. When you attend to all the words in the input, its called global or soft attention. On the other hand, when the attention mechanism focuses on a small window of context, its called local attention. Hard attention is a modification to local, where the window size is one ie. we focus only on one hidden state to generate the context vector. While less expensive during inference, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. We will explore this topic in detail later.</p>

<h3 id="self-attention">Self-Attention</h3>

<p>One of the most important building block leading up-to the transformer is self-attention introduced by <d-cite key="lin2017structured"></d-cite> in 2017 as a way to replace the max pooling or averaging step in sequential models. Instead of a single vector representation, this mechanism is performed on top of a sequence model extracting multiple vector representations based on the hidden states of the LSTM. This is explained with an example (Fig. 1) as follows from <d-cite key="lin2017structured"></d-cite>. Of the two parts in this model, the first is a bidirectional LSTM (not covered here) and the second is the self-attention mechanism which we will cover in detail (section 2.1 of the paper).</p>

<p>Let the sentence <code class="language-plaintext highlighter-rouge">S</code> have <code class="language-plaintext highlighter-rouge">n</code> tokens represented by their embeddings \(w_i\) which is a <code class="language-plaintext highlighter-rouge">d</code> dimensional vector. Thus <code class="language-plaintext highlighter-rouge">S</code> can be represented as matrix with dimension \(n \times d\). Let \(h_i\) be the concatenated hidden state of the LSTM with <code class="language-plaintext highlighter-rouge">2u</code> dimensions. Therefore, <code class="language-plaintext highlighter-rouge">H</code> is a matrix of size \(n \times 2u\).</p>

<p>\begin{equation}
S = (w_1, w_2, … , w_n)
\end{equation}</p>

<p>\begin{equation}
H = (h_1, h_2, … , h_n)
\end{equation}</p>

<div class="col-sm mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-self-attention-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-self-attention-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-self-attention-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-self-attention-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<div class="caption">
    Fig. Input embedding and LSTM hidden vectors
</div>

<p>The aim is to encode this variable sequence into a fixed size embedding. Unlike the previous attention mechanism where we used the decoder hidden state to compute the context vector, here we only use the input hidden states. This is achieved through two linear transformations shown below</p>

<div class="col-sm mt-3 mt-md-0">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-self-attention-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-self-attention-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-self-attention-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-self-attention-2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<div class="caption">
    Fig. Self attention mechanism for single and multiple annotation vector
</div>

<p>The image to the left above shows the transformations for a single annotation vector <code class="language-plaintext highlighter-rouge">a</code> and the image to the right is for the annotation matrix <code class="language-plaintext highlighter-rouge">A</code>. The only difference between the two is the we use vector \(w_{s2}\) for single attention and matrix \(W_{S2}\) for multiple attention to focus on different parts of the sentence. This is represented by the following equations in the paper</p>

<p>\begin{equation}
a = softmax(w_{s2}\ tanh(W_{s1}\ H^T))
\end{equation}</p>

<p>\begin{equation}
A = softmax(W_{s2}\ tanh(W_{s1}\ H^T))
\end{equation}</p>

<p>The dimensions of the output during the above transformations are shown in the fig above. We get the embedding vector <code class="language-plaintext highlighter-rouge">m</code> (\(1 \times 2u\)) and embedding matrix <code class="language-plaintext highlighter-rouge">M</code> (\(r \times 2u\)) by taking the weighted sum of <code class="language-plaintext highlighter-rouge">H</code> using <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">A</code> respectively. To solve for redundancy problems the paper mentions a penalization term (section 2.2) which we will not cover here. This builds up to the seminal paper Attention Is All You Need <d-cite key="vaswani2017attention"></d-cite> which proposed the <b>Transformer</b>, a network architecture based only on attention mechanism.</p>

<h2 id="transformers">Transformers</h2>

<p>Introduced in 2017, this topic needs no introduction so we will get right to it. Using our understanding of the attention mechanism and sequence-2-sequence networks covered so far, we look at the Scaled Dot-Product Attention and Multi-Head Attention (orange block in the fig. below) used in Transformers. Self-attention, as defined is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>

<div class="row justify-content-md-center">
    <div class="col col-6">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-transformer-attention-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-transformer-attention-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-transformer-attention-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-transformer-attention-1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Fig. Transformer architecture from <d-cite key="vaswani2017attention"></d-cite>
</div>

<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>

<p>The attention function (shown below) can be defined as mapping a query and a set of key-value pairs to an output where the output is a weighted sum of the values with weights computed by a function of the query and corresponding key.</p>

<p>\begin{equation}
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}</p>

<p>The inputs are key and query vectors of dimension \(d_k\) and value of dimension \(d_v\). The following operations are performed:</p>
<ul>
  <li>Take the dot product of \(Q\) and \(K^T\)</li>
  <li>Scale it by \(\frac{1}{\sqrt{d_k}}\)</li>
  <li>Apply softmax to the scaled dot product to obtain weights</li>
  <li>Compute linear combination of (\(V\)) and weights</li>
</ul>

<div class="row justify-content-md-center">
    <div class="col col-12">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-transformer-2.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-transformer-2.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-transformer-2.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-transformer-2.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Fig. Scaled Dot-Product Attention 
</div>

<p>How do you generate the \(Q, K, V\) matrices? The output from the embedding layer is \(n\) tokens of size \(d_{model}\). To convert to \(d_k\) we linearly project the embedding vector by a matrix of size \(d_{model} \times d_k\) shown below. In practice, we group the input vectors before linearly projecting them to obtain the \(Q, K, V\) matrices.</p>

<div class="row justify-content-md-center">
    <div class="col col-12">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-transformer-1.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-transformer-1.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-transformer-1.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-transformer-1.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Fig. Generating Q, K and V from input embedding
</div>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>Here we have multiple (\(h\) ie. number of heads) Scaled Dot-Product Attention blocks running in parallel. Thus we project the queries, keys and values \(h\) times with different learned linear projections to \(d_k, d_k, d_v\) dimensions. This is shown in the figure above.</p>

<p>\begin{equation}
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}</p>

<p>where \(W_i^Q \in \mathbb{R}^{d_{model} \times d_k}\), \(W_i^K \in \mathbb{R}^{d_{model} \times d_k}\) and \(W_i^V \in \mathbb{R}^{d_{model} \times d_v}\). Depending on \(h\) (number of heads), \(d_k = \frac{d_{model}}{h}\). In the paper, \(h = 8\) and \(d_k = d_v\).</p>

<p>Once you have the outputs from each head, you concatenate them and perform a linear projection as follows</p>

<p>\begin{equation}
MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O
\end{equation}</p>

<p>where \(W^O \in \mathbb{R}^{hd_v \times d_{model}}\). This is shown below</p>

<div class="row justify-content-md-center">
    <div class="col col-12">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/b1-transformer-3.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/b1-transformer-3.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/b1-transformer-3.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/b1-transformer-3.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Fig. Multi-Head Attention
</div>

<ul>
  <li>Since each head only works on part of the input, the total computational cost is same as that of single-head attention with full dimensionality.</li>
  <li>To prevent the leftward flow of information and to preserve the auto-regressive property, all the illegal connections are set to \(-\infty\) pre-softmax.</li>
</ul>

<p>For additional details regarding positional encoding, encoder-decoder, training etc. please refer to <d-cite key="vaswani2017attention"></d-cite>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The domain of attention and transformer based architectures is a rapidly evolving field with new techniques published at lightning speed so it’s impossible to cover everything everywhere all at once. Certainly read the papers mentioned in the references below to dive deeper on this topic. I hope this read has been worth your time. Please do reach out with feedback and questions if any. Thanks!</p>]]></content><author><name></name></author><category term="attention" /><category term="deep-learning" /><category term="transformers" /><category term="neural-machine-translation" /><summary type="html"><![CDATA[Understanding attention mechanism in sequence to sequence networks]]></summary></entry></feed>